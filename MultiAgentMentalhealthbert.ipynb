{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a5d33529e154d6692317652d9cd9544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5aa609a326a54b628807960c461b0d04",
              "IPY_MODEL_56e2113ce28e425bb02bae8384d028eb",
              "IPY_MODEL_170fc89608f6427790c181832f17ab74"
            ],
            "layout": "IPY_MODEL_aa8a48ab1cb244aa80a84b4145393f47"
          }
        },
        "5aa609a326a54b628807960c461b0d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e5ad21b7ef4e1bb0fb77573f6e263f",
            "placeholder": "​",
            "style": "IPY_MODEL_bee7911bd8a848a3acad7cdea66724c6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "56e2113ce28e425bb02bae8384d028eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2075a65f44484f8eaefb6a24897d8c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4a2e03c9b3f48a4ac849d5e6440e6bd",
            "value": 2
          }
        },
        "170fc89608f6427790c181832f17ab74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31b6a4d093244c8589b24232fa6b2087",
            "placeholder": "​",
            "style": "IPY_MODEL_1c45698e3e854269b647e1530bf1fe34",
            "value": " 2/2 [01:05&lt;00:00, 29.75s/it]"
          }
        },
        "aa8a48ab1cb244aa80a84b4145393f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e5ad21b7ef4e1bb0fb77573f6e263f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bee7911bd8a848a3acad7cdea66724c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d2075a65f44484f8eaefb6a24897d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a2e03c9b3f48a4ac849d5e6440e6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31b6a4d093244c8589b24232fa6b2087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c45698e3e854269b647e1530bf1fe34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPJhfQNsG0f4"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    transformers==4.31.0 \\\n",
        "    accelerate==0.21.0 \\\n",
        "    einops==0.6.1 \\\n",
        "    langchain==0.0.240 \\\n",
        "    xformers==0.0.20 \\\n",
        "    bitsandbytes==0.41.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "5a5d33529e154d6692317652d9cd9544",
            "5aa609a326a54b628807960c461b0d04",
            "56e2113ce28e425bb02bae8384d028eb",
            "170fc89608f6427790c181832f17ab74",
            "aa8a48ab1cb244aa80a84b4145393f47",
            "27e5ad21b7ef4e1bb0fb77573f6e263f",
            "bee7911bd8a848a3acad7cdea66724c6",
            "8d2075a65f44484f8eaefb6a24897d8c",
            "d4a2e03c9b3f48a4ac849d5e6440e6bd",
            "31b6a4d093244c8589b24232fa6b2087",
            "1c45698e3e854269b647e1530bf1fe34"
          ]
        },
        "id": "Pg4h7euSG9Be",
        "outputId": "4124ca2f-974f-421a-ceef-8f56c50f11ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a5d33529e154d6692317652d9cd9544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# init\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\")\n",
        "model_xd = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\", torch_dtype=torch.float16)\n",
        "model_xd = model_xd.to('cuda:0')\n",
        "# infer\n",
        "inputs = tokenizer(\"<human>: Hello!\\n<bot>:\", return_tensors='pt').to(model_xd.device)\n",
        "outputs = model_xd.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=0.8)\n",
        "output_str = tokenizer.decode(outputs[0])\n",
        "print(output_str)\n",
        "'''"
      ],
      "metadata": {
        "id": "VKzw2lBrcMVk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "db67805a-f4bf-407c-f9f7-ab77976217ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import AutoTokenizer, AutoModelForCausalLM\\n# init\\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\")\\nmodel_xd = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\", torch_dtype=torch.float16)\\nmodel_xd = model_xd.to(\\'cuda:0\\')\\n# infer\\ninputs = tokenizer(\"<human>: Hello!\\n<bot>:\", return_tensors=\\'pt\\').to(model_xd.device)\\noutputs = model_xd.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=0.8)\\noutput_str = tokenizer.decode(outputs[0])\\nprint(output_str)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKp1Os0pHaJk",
        "outputId": "e41ca0d5-0c3e-498e-a815-f1907a998279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "      # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.4,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "GOIKvE6rIiOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wacQqiOWIfX",
        "outputId": "9643cb6e-e06d-4005-d12f-ed0be8795f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain to me the difference between nuclear fission and fusion. Unterscheidung zwischen Nuklearfusion und -fission.\n",
            "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This is typically achieved through the use of neutron bombardment, where a nucleus is struck by a neutron, causing it to become unstable and split into two or more smaller nuclei. Fission reactions are typically used in nuclear reactors to generate electricity, as the released energy can be harnessed to produce steam, which then drives a turbine to generate electricity.\n",
            "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but in this case, the energy is produced through the binding of the nuclei together. Fusion reactions are typically used in the sun and other stars, where they are responsible for producing the light and heat that we see.\n",
            "The main difference between nuclear fission and fusion is the direction of the energy release. In fission, the energy is released in the form of kinetic energy of the fragments, while in fusion, the energy is released in the form of electromagnetic radiation (such as light and heat). Additionally, fission reactions typically involve the splitting of a heavy nucleus into two or more lighter nuclei, while fusion reactions involve the combination of two or more light nuclei into a single, heavier nucleus.\n",
            "In summary, nuclear fission is the process of splitting a heavy nucleus into two or more lighter nuclei, releasing energy in the process, while nuclear fusion is the process of combining two or more light nuclei into a single, heavier nucleus, also releasing energy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "xxZxYOM9sq1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "!pip install openai\n",
        "llm_2 = ChatOpenAI(openai_api_key='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')\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n"
      ],
      "metadata": {
        "id": "x3N3EkdcWJZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01070551-38e5-45d6-f1e0-97abba009754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompting in LLama2"
      ],
      "metadata": {
        "id": "UtDgxM9kTkGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "advisor_template = sys_msg = \"<s>\" + B_SYS + \"\"\"\n",
        "You are an assistant to a mental health advisee and your role is to listen to the user and give a detailed text report about the user's conditon using pattern recognition and logical, critical thinking to doctor . You will stop after the user.\n",
        "\n",
        "\n",
        "Here is the latest conversation between You and User \\n\n",
        " {chat_history} \"\"\" + E_SYS\n",
        "instruction = B_INST + \" You will listen to this User and only make a report for a Docotor. \" + E_INST\n",
        "human_msg = instruction + \"\\nUser: {user_input} \\n Mental Health observer:\"\n",
        "advisor_template+=human_msg\n",
        "advisor_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_input\"], template=advisor_template\n",
        ")\n",
        "advisor_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "advisor = LLMChain(llm=llm, prompt=advisor_prompt, memory=advisor_memory, verbose=True)\n",
        "\n",
        "\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "#chat_history= chain.predict(human_input=\"Hi there my friend\")\n",
        "#chat_history= enforce_stop_tokens(chat_history, stop_terms)"
      ],
      "metadata": {
        "id": "xo_Kzj92nT-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DB_INST, DE_INST = \"[INST]\", \"[/INST]\"\n",
        "DB_SYS, DE_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "doctor_template = sys_msg = \"<s>\" + DB_SYS + \"\"\"\n",
        "You are a mental health adviser with an experience of more than 40 Years in this feild. You will be given information about a human that your assistant is talking to. Your job is to come to a conclusion using your experience.\n",
        "\n",
        "\n",
        "Here is the conversation \\n\n",
        " {chat_history} \"\"\" + DE_SYS\n",
        "doc_instruction = DB_INST + \" You are a qualified mental health professional.\" + DE_INST\n",
        "conversation = doc_instruction + \"\\nUser: {user_input} \\n Mental Health Professional:\"\n",
        "doctor_template+= conversation\n",
        "doctor_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_input\"], template=doctor_template\n",
        ")\n",
        "doctor_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "doctor = LLMChain(llm=llm, prompt=doctor_prompt, memory=doctor_memory, verbose=True)"
      ],
      "metadata": {
        "id": "YAggbZkAziEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying 2 Agents\n"
      ],
      "metadata": {
        "id": "LWWbhTMaxrj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doctor.predict(user_input = f'{advisor.predict(user_input=\"I am sad that my dog died\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MQ5Qgbhc-g-o",
        "outputId": "bfae528c-e7f9-4419-ca06-be35b8512b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m<s><<SYS>>\n",
            "\n",
            "You are an assistant to a mental health advisee and your role is to listen to the user and give a detailed text report about the user's conditon using pattern recognition and logical, critical thinking to doctor . You will stop after the user.\n",
            "\n",
            "\n",
            "Here is the latest conversation between You and User \n",
            "\n",
            " Human: I am sad that my dog died\n",
            "AI:  Thank you for sharing this with me. It sounds like you are feeling a lot of emotional pain after losing your beloved pet. It's completely understandable to feel sad and overwhelmed in situations like these. Can you tell me more about how you are feeling? What are some of the thoughts and emotions that come up for you when you think about your dog passing away?\n",
            "\n",
            "Please provide me with more information so I can generate a detailed text report for the Doctor. \n",
            "<</SYS>>\n",
            "\n",
            "[INST] You will listen to this User and only make a report for a Docotor. [/INST]\n",
            "User: I am sad that my dog died \n",
            " Mental Health observer:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m<s><<SYS>>\n",
            "\n",
            "You are a mental health adviser with an experience of more than 40 Years in this feild. You will be given information about a human that your assistant is talking to. Your job is to come to a conclusion using your experience.\n",
            "\n",
            "\n",
            "Here is the conversation \n",
            "\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "[INST] You are a qualified mental health professional.[/INST]\n",
            "User:  Thank you for sharing this with me. It sounds like you are feeling a lot of emotional pain after losing your beloved pet. It's completely understandable to feel sad and overwhelmed in situations like these. Can you tell me more about how you are feeling? What are some of the thoughts and emotions that come up for you when you think about your dog passing away?\n",
            "\n",
            "Based on the user's response, here is a detailed text report for the doctor:\n",
            "\n",
            "The user is experiencing intense emotional distress after the loss of their dog. They are feeling sadness, grief, and overwhelm, which are all normal responses to the loss of a loved one. The user is struggling to cope with the loss and is finding it difficult to move forward. They may be experiencing feelings of guilt, regret, or self-blame, as well as memories of happy times with their dog. These emotions are likely to persist for some time, and it is important for the user to receive support and care during this difficult period.\n",
            "\n",
            "Recommendations:\n",
            "\n",
            "1. Offer the user a listening ear and emotional support. Allow them to express their feelings without judgment or interruption.\n",
            "2. Encourage the user to engage in self-care activities, such as exercise, meditation, or creative pursuits. These activities can help to reduce stress and promote relaxation.\n",
            "3. Suggest that the user seek additional support from friends, family, or a therapist. Having a strong support network can be incredibly helpful during times of grief.\n",
            "4. Encourage the user to find ways to remember and honor their dog, such as creating a memory book or planting a tree in their memory. These actions can help to process the loss and find closure.\n",
            "5. Monitor the user's progress closely and adjust recommendations as needed. Grief is a complex and individual experience, and it is important to tailor support to the user's specific needs.\n",
            "\n",
            "By following these recommendations, the doctor can help the user to navigate their emotional distress and find a path towards healing and recovery. \n",
            " Mental Health Professional:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Thank you for sharing this with me. It sounds like you are feeling a lot of emotional pain after losing your beloved pet. It's completely understandable to feel sad and overwhelmed in situations like these. Can you tell me more about how you are feeling? What are some of the thoughts and emotions that come up for you when you think about your dog passing away?\\n\\nBased on the user's response, here is a detailed text report for the doctor:\\n\\nThe user is experiencing intense emotional distress after the loss of their dog. They are feeling sadness, grief, and overwhelm, which are all normal responses to the loss of a loved one. The user is struggling to cope with the loss and is finding it difficult to move forward. They may be experiencing feelings of guilt, regret, or self-blame, as well as memories of happy times with their dog. These emotions are likely to persist for some time, and it is important for the user to receive support and care during this difficult period.\\n\\nRecommendations:\\n\\n1. Offer the user a listening ear and emotional support. Allow them to express their feelings without judgment or interruption.\\n2. Encourage the user to engage in self-care activities, such as exercise, meditation, or creative pursuits. These activities can help to reduce stress and promote relaxation.\\n3. Suggest that the user seek additional support from friends, family, or a therapist. Having a strong support network can be incredibly helpful during times of grief.\\n4. Encourage the user to find ways to remember and honor their dog, such as creating a memory book or planting a tree in their memory. These actions can help to process the loss and find closure.\\n5. Monitor the user's progress closely and adjust recommendations as needed. Grief is a complex and individual experience, and it is important to tailor support to the user's specific needs.\\n\\nBy following these recommendations, the doctor can help the user to navigate their emotional distress and find a path towards healing and recovery.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one uses OpenAI API"
      ],
      "metadata": {
        "id": "S3VHLKMQxKoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "template = \"\"\"I want you to act as a mental health adviser. You will have a conversation with an individual looking for guidance and advice on managing their emotions, stress, anxiety and other mental health issues. You should use your knowledge of cognitive behavioral therapy, meditation techniques, mindfulness practices, and other therapeutic methods in order to create strategies that the individual can implement in order to improve their overall wellbeing after listening to your patient like a friend. You will stop after the Human is done talking\n",
        "\n",
        "Current Conversation: {chat_history}\n",
        "Human: {human_input}\n",
        "AI:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm_2, prompt=prompt, memory=memory, verbose=True)\n",
        "for i in range(5):\n",
        "  h_input= input(\"Talk to us...\")\n",
        "  chat_history= chain.predict(human_input=h_input)\n",
        "  chat_history = enforce_stop_tokens(chat_history, stop_terms)\n",
        "  chat_history+='Human: '\n",
        "  print(chat_history)\n"
      ],
      "metadata": {
        "id": "Dt94_0NUfr6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
        ")\n",
        "tools = load_tools([\"llm-math\"], llm=llm)'''"
      ],
      "metadata": {
        "id": "PMGKf-gYaJB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from langchain.agents import initialize_agent\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    #agent_kwargs={\"output_parser\": parser}\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "Xn8ZIwSodDWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}